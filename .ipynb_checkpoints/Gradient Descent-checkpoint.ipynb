{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an algorithm that minimizes some objective $J(\\theta)$ where $\\theta$ is $\\in \\mathbb{R}^d$ by iteratively updating the vector $\\theta$ by moving in the direction of steepest descent multiplied by some step size or learning rate.\n",
    "\n",
    "In essence, \n",
    "\n",
    "$\\>\\>\\>\\>$1: Pick an initial guess $\\theta_0$    \n",
    "$\\>\\>\\>\\>$2: $\\>\\>$while $k \\in [0...iteration_{max}]$ and $\\theta_{k+1} - \\theta_{k} \\leq precision$:    \n",
    "$\\>\\>\\>\\>$3: $\\>\\>\\>\\>\\>\\>\\>\\>$ $\\theta_{k+1}$ = $\\theta_{k} -\\nabla J_\\theta(\\theta_k)$ * $\\alpha$.\n",
    "\n",
    "This is relevant to Machine Learning because it offers a numerical solution to estimates of parameters that do not have a closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Multinomial Logistic Regression Parameters with Gradient Descent \n",
    "\n",
    "However, one needs to determine the objective $J(\\theta)$ before executing this algorithm. In the case of linear regression, the sum of squared residuals could be used. In this case, one can use the idea of MLE to minimize the log liklihood.\n",
    "\n",
    "In the case where $K = 2$, the likelihood function was $\\Pi_{i=1}^n p(\\textbf x_i)^{y_i} (1-p(\\textbf x_i))^{1-y_i}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where $K > 2$,\n",
    "\n",
    "For $k \\in K$ where $K$ is the set of labeled classes and $p$ is the number of features      \n",
    "Let $p_k(\\textbf x)$ be the posterior probability $P(Y = k | X = \\bf x)$ and the model be defined as \n",
    "\n",
    "$p_k(\\textbf x) = \\frac{e^{\\beta_{0}^k + \\textbf w_k\\cdot \\textbf x}}{\\sum_{k=1}^{K} e^{\\beta_{0}^k + \\textbf w_k\\cdot \\textbf x}}$ for $k=1...K$\n",
    "\n",
    "One can see how each class $1 \\leq k \\leq K$ has its own set of parameters $w_k = (\\beta_1,...,\\beta_p)$\n",
    "\n",
    "We want the optimal set of parameters(weights): argmax $L(\\beta$ $|$ $ \\textbf x)$\n",
    "\n",
    "$$\\prod_{i=1}^{N}\\prod_{k=1}^{K}P(Y = k | X = \\textbf x_i)^{k_i} = \\prod_{i=1}^{N}\\prod_{k=1}^{K} p_k(\\textbf x_i)^{k_i}$$\n",
    "\n",
    "Minimizing the negative log-liklihood is the same as maximizing the log-liklihood. \n",
    "\n",
    "$$J(\\textbf w) = -log\\prod_{i=1}^{N}\\prod_{k=1}^{K} p_k(\\textbf x_i)^{k_i} = -\\sum_{i=1}^{N}\\sum_{k=1}^{K} {k_i} * log(\\hat{y}_{ki})$$\n",
    "\n",
    "where $k_i$ is 1 if $\\textbf x_i$ belongs to the class k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGradientDescent:\n",
    "    def _init_(self):\n",
    "        self.rate = .01\n",
    "        self.precision = .001\n",
    "    \n",
    "    def gradient(self):\n",
    "        return 0\n",
    "    \n",
    "    def determineCost(self):\n",
    "        return 0\n",
    "    def gradientDescent(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def addBias(self):\n",
    "        return 0\n",
    "    \n",
    "    def normalize(self):\n",
    "        return 0\n",
    "    \n",
    "    def train(self,trainX,trainy):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# loading the mnist dataset\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
