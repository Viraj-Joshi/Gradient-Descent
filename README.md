# Gradient Descent


This project will serve as an opportunity to implement Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent.

One this is done, and relevant comparisons between the three are made, optimizations such as momentum and Nesterov accelerated gradient are TODOs.


