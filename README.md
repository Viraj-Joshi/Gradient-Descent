# Gradient Descent


This project will serve as an opportunity to implement and explain Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent.

Optimizations such as momentum, Nesterov accelerated gradient were made.

TODO: implement RMSProp and Adagrad.

### Click this for the notebook: https://nbviewer.jupyter.org/github/Viraj-Joshi/Gradient-Descent/blob/master/Gradient%20Descent.ipynb

Implementing ideas found in: Sebastian Ruder (2016). An overview of gradient descent optimisation algorithms. arXiv preprint arXiv:1609.04747

