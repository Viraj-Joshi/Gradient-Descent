# Gradient Descent


This project will serve as an opportunity to implement and explain Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent.

Optimizations such as momentum, Nesterov accelerated gradient were made.

TODO: implement RMSProp and Adagrad.

Implementing ideas found in: Sebastian Ruder (2016). An overview of gradient descent optimisation algorithms. arXiv preprint arXiv:1609.04747

