# Gradient Descent


This project will serve as an opportunity to implement Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent.

Once this is done, and relevant comparisons between the three are made, optimizations such as momentum and Nesterov accelerated gradient are will be made.

TODO: Derivation of gradient and explanations for acceleration and momentum.

With time, do ADAGRAD.

Implementing ideas found in: Sebastian Ruder (2016). An overview of gradient descent optimisation algorithms. arXiv preprint arXiv:1609.04747

