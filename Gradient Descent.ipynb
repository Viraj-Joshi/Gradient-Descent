{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an algorithm that minimizes some objective $J(\\theta)$ where $\\theta$ is $\\in \\mathbb{R}^d$ by iteratively updating the vector $\\theta$ by moving in the direction of steepest descent multiplied by some step size or learning rate.\n",
    "\n",
    "In essence, \n",
    "\n",
    "$\\>\\>\\>\\>$1: Pick an initial guess $\\theta_0$    \n",
    "$\\>\\>\\>\\>$2: $\\>\\>$while $\\theta_{k+1} - \\theta_{k} \\leq precision$:    \n",
    "$\\>\\>\\>\\>$3: $\\>\\>\\>\\>\\>\\>\\>\\>$ $\\theta_{k+1}$ = $\\theta_{k} -\\nabla J_\\theta(\\theta_k)$ * $\\alpha$.\n",
    "\n",
    "This is relevant to Machine Learning because it offers a numerical solution to estimates of parameters that do not have a closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Multinomial Logistic Regression Parameters with Gradient Descent \n",
    "\n",
    "#### Cost Function\n",
    "However, one needs to determine the objective $J(\\theta)$ before executing this algorithm. In the case of linear regression, the sum of squared residuals could be used. In this case, one can use the idea of MLE to minimize the log liklihood.\n",
    "\n",
    "In the case where $K = 2$, the likelihood function was $\\prod_{i=1}^{n} p(\\textbf x_i)^{y_i} (1-p(\\textbf x_i))^{1-y_i}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where $K > 2$,\n",
    "\n",
    "For $k \\in K$ where $K$ is the set of labeled classes and $p$ is the number of features      \n",
    "Let $p_k(\\textbf x)$ be the posterior probability $P(Y = k | X = \\bf x)$ and the model be defined as \n",
    "\n",
    "$p_k(\\textbf x) = \\frac{e^{\\beta_{0}^k + \\textbf w_k\\cdot \\textbf x}}{\\sum_{k=1}^{K} e^{\\beta_{0}^k + \\textbf w_k\\cdot \\textbf x}}$ for $k=1...K$\n",
    "\n",
    "One can see how each class $1 \\leq k \\leq K$ has its own set of parameters $w_k = (\\beta_1,...,\\beta_p)$\n",
    "\n",
    "We want the optimal set of parameters(weights): argmax $L(\\beta$ $|$ $ \\textbf x)$\n",
    "\n",
    "$$\\prod_{i=1}^{N}\\prod_{k=1}^{K}P(Y = k | X = \\textbf x_i)^{k_i} = \\prod_{i=1}^{N}\\prod_{k=1}^{K} p_k(\\textbf x_i)^{k_i}$$\n",
    "\n",
    "Minimizing the negative log-liklihood is the same as maximizing the log-liklihood. \n",
    "\n",
    "$$J(\\textbf w) = -log\\prod_{i=1}^{N}\\prod_{k=1}^{K} p_k(\\textbf x_i)^{k_i} = -\\sum_{i=1}^{N}\\sum_{k=1}^{K} {k_i} * log(\\hat{y}_{ki})$$\n",
    "\n",
    "where $k_i$ is 1 if $\\textbf x_i$ is labeled as class k\n",
    "\n",
    "Finally, we introduce the L2 norm to obtain a regularized objective function.\n",
    "\n",
    "$$J(\\textbf w) = -log\\prod_{i=1}^{N}\\prod_{k=1}^{K} p_k(\\textbf x_i)^{k_i} = -\\sum_{i=1}^{N}\\sum_{k=1}^{K} {k_i} * log(\\hat{y}_{ki}) + \\alpha R(\\beta)$$\n",
    "\n",
    "where $R(\\beta) = ||\\beta||^2_2 = \\sum^{k}_{i=1}\\beta_i^2$ and $p_k(\\textbf x_i)^{k_i} = \\hat{y}_{ki}$\n",
    "\n",
    "\n",
    "#### Gradient of Cost Function\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGradientDescent:\n",
    "    def __init__(self):\n",
    "        self.rate = .01\n",
    "        self.precision = .001\n",
    "        self.reg = .001\n",
    "    \n",
    "    # Pre: X - design matrix\n",
    "    #      y - labeled classes\n",
    "    #      w - mxn parameter matrix of current model\n",
    "    # Post: a vector of errors for each label\n",
    "    def determineCost(self, X, y, w):\n",
    "        yhat = softmax(np.dot(X,w))\n",
    "        \n",
    "        # sum k sets of squared beta_i\n",
    "        r_theta = self.reg * np.sum(w * w, axis = 0)\n",
    "        # * operator will ensure a valid summation over all samples for each label because the labels have been one-hot encoded\n",
    "        J = -np.sum((y * np.log(yhat)),axis = 0)+r_theta\n",
    "        return J\n",
    "    \n",
    "    # Post: the gradient of J evaluated at w\n",
    "    def gradient(self, X, y, w):\n",
    "        yhat = softmax(np.dot(X,w))\n",
    "        print(np.dot(X,yhat-y))\n",
    "        return 0\n",
    "    \n",
    "    # Pre: z - x dot w of dimension (n*p)multiply(p*k)=>n*k\n",
    "    # Post: Softmax score for each observation\n",
    "    def softmax(self,z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "    def gradientDescent(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "class LogisticRegression:\n",
    "    \n",
    "    # Pre - data matrix X\n",
    "    # Post - data matrix X with bias term concatenated as first column \n",
    "    def addBias(self,X):\n",
    "        X = np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
    "        return X\n",
    "    \n",
    "    def normalize(self,X):                                                     \n",
    "        X = (X - np.min(X))/float(np.max(X) - np.min(X) )\n",
    "        return X\n",
    "    \n",
    "    def oneHotEncode(self,y):\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(y)\n",
    "        return enc.transform(y).toarray()\n",
    "    \n",
    "    def train(self,trainX,trainy):\n",
    "        \n",
    "        trainX = self.addBias(trainX)\n",
    "        trainX = self.normalize(trainX)\n",
    "        trainy = self.oneHotEncode(trainy)\n",
    "        \n",
    "        p,k = trainX.shape[1],trainy.shape[1]\n",
    "        w = np.random.random([p*k]).reshape((p,k))\n",
    "        \n",
    "        GD = BatchGradientDescent()\n",
    "        print(GD.determineCost(trainX,trainy,w))\n",
    "        GD.gradient(trainX,trainy,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 784), y=(60000, 1)\n",
      "Test: X=(10000, 784), y=(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# loading the mnist dataset\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "\n",
    "trainX = trainX.reshape((trainX.shape[0],28*28))\n",
    "trainy = trainy.reshape((trainX.shape[0],1))\n",
    "\n",
    "testX = testX.reshape((testX.shape[0],28*28))\n",
    "testy = testy.reshape((testX.shape[0],1))\n",
    "\n",
    "# summarize loaded and reshaped dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26542.55932035 26413.09405568 14741.14381946 30530.23747941\n",
      " 36951.18949817 21313.4603345  50821.70221678 24627.02794491\n",
      " 21866.27045249 19360.46238479]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (60000,785) and (60000,10) not aligned: 785 (dim 1) != 60000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-848af41fdd60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-130-e0e64fb5fb99>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, trainX, trainy)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchGradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetermineCost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-134-df62441eba9d>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, X, y, w)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (60000,785) and (60000,10) not aligned: 785 (dim 1) != 60000 (dim 0)"
     ]
    }
   ],
   "source": [
    "LogisticRegression().train(trainX,trainy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
